{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the PCA Transformation"
      ],
      "metadata": {
        "id": "mwAblZNjfdiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA follows the fit/transform pattern**"
      ],
      "metadata": {
        "id": "qe8K8V0Oh3uO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YJGfkJGfNu1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "PCA (Principal Component Analysis) in scikit-learn works in two steps:\n",
        "1) Fit: Learns how to shift and rotate the data to find the best lower-dimensional representation.\n",
        "2) Transform: Actually applies this transformation to the data, reducing its dimensions.\n",
        "\n",
        "Example to Make It Clear\n",
        "A dataset of students' grades in Math, Science, and English.\n",
        "want to reduce the number of features while keeping most of the important information.\n",
        "\n",
        "Step 1: Fit\n",
        "----------------\n",
        "PCA analyzes the data and figures out how to rotate it so that the most important patterns appear first.\n",
        "This step does not change the data—it just learns how to transform it.\n",
        "\n",
        "\n",
        "Step 2: Transform\n",
        "------------------\n",
        "Now, PCA applies the learned transformation to actually reduce the dimensions.\n",
        "For example, instead of having 3 features (Math, Science, English), it might reduce it to just 2 principal components.\n",
        "\n",
        "\n",
        "New, unseen data (e.g., a new student’s grades) can also be transformed using the same learned transformation!\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA Features**"
      ],
      "metadata": {
        "id": "M4sUfBzziiGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Step 1: Import PCA and Fit the Data\n",
        "------------------------------------------\n",
        "1) First, import PCA from sklearn.decomposition.\n",
        "2) Then, create a PCA object and fit it to your dataset. This step learns how to reduce the dimensions.\n",
        "3) Finally, transform the dataset using the trained PCA object. This returns a new dataset with PCA features.\n",
        "\n",
        "A wine dataset with features like:\n",
        "\n",
        " Alcohol content\n",
        " Sugar level\n",
        " Acidity\n",
        "\n",
        "If we apply PCA, it creates new features (PCA features) that combine information from the original features.\n",
        "\n",
        "Step 2: Understanding the Transformed Data\n",
        "-------------------------------------------------\n",
        " The transformed dataset still has the same number of rows (one for each wine sample).\n",
        " But the columns now represent PCA features instead of the original features like alcohol or acidity.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q8jOtbSOikGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA features are not correlated**"
      ],
      "metadata": {
        "id": "1FkFpFPPjZ_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "1) PCA Removes Correlation Between Features\n",
        "In many datasets, some features are correlated, meaning they change together.\n",
        "\n",
        "In a wine dataset\n",
        "  Alcohol % and Sugar Level might be correlated (higher alcohol means lower sugar).\n",
        "  Acidity and Sweetness might also be related.\n",
        "\n",
        "But PCA transforms the data in such a way that the new features (PCA features) are not correlated anymore.\n",
        "This happens because PCA rotates the data in a way that spreads the important information across new axes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PCA is designed to reduce redundancy in data. Correlated features contain overlapping information, meaning some features are not adding much new insight.\n",
        "PCA transforms the data so that each new feature (PCA component) captures unique patterns in the dataset.\n",
        "\n",
        "\n",
        "Suppose while analyzing students' performance, and we have \"Hours of Study\" and \"Number of Pages Read\" as features.\n",
        "These two features are highly correlated because studying more often means reading more pages.\n",
        "PCA will combine these into a single \"Study Effort\" component, reducing redundancy.\n",
        "\n",
        "\n",
        "\n",
        "When Correlation is Important (When NOT to Use PCA)\n",
        "---------------------------------------------------------\n",
        "If we need to interpret relationships between features, PCA might not be the best choice.\n",
        "Some ML models, like Decision Trees or XGBoost, work well with correlated features.\n",
        "If correlation is meaningful (e.g., stock market relationships, weather patterns), PCA could remove valuable insights.\n",
        "\n",
        "\n",
        "When PCA is Useful\n",
        " Dimensionality Reduction – If have too many features, PCA helps simplify the dataset.\n",
        " Noise Reduction – It removes unnecessary variations.\n",
        " Visualization – If data has many dimensions (e.g., 100 features), PCA helps convert it to 2D or 3D for easy visualization.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_8cuIFq1jaN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import:\n",
        "matplotlib.pyplot as plt.\n",
        "pearsonr from scipy.stats.\n",
        "\n",
        "\n",
        "Assign column 0 of grains to width and column 1 of grains to length.\n",
        "Make a scatter plot with width on the x-axis and length on the y-axis.\n",
        "Use the pearsonr() function to calculate the Pearson correlation of width and length\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assign the 0th column of grains: width\n",
        "width = grains[:, 0]\n",
        "\n",
        "# Assign the 1st column of grains: length\n",
        "length = grains[:, 1]\n",
        "\n",
        "# Scatter plot width vs length\n",
        "plt.scatter(width, length)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation\n",
        "correlation, pvalue = pearsonr(width, length)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)\n",
        "\n",
        "### 0.8604149377143469\n"
      ],
      "metadata": {
        "id": "dwM9bYGjyBNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Decorrelating the grain measurements with PCA\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "You observed in the previous exercise that the width and length measurements of the grain are correlated. Now, you'll use PCA to decorrelate these measurements,\n",
        "then plot the decorrelated points and measure their Pearson correlation\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import PCA from sklearn.decomposition.\n",
        "Create an instance of PCA called model.\n",
        "Use the .fit_transform() method of model to apply the PCA transformation to grains. Assign the result to pca_features.\n",
        "The subsequent code to extract, plot, and compute the Pearson correlation of the first two columns pca_features has been written for you, so hit submit to see the result!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Apply the fit_transform method of model to grains: pca_features\n",
        "pca_features = model.fit_transform(grains)\n",
        "\n",
        "# Assign 0th column of pca_features: xs\n",
        "xs = pca_features[:,0]\n",
        "\n",
        "# Assign 1st column of pca_features: ys\n",
        "ys = pca_features[:,1]\n",
        "\n",
        "# Scatter plot xs vs ys\n",
        "plt.scatter(xs, ys)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation of xs and ys\n",
        "correlation, pvalue = pearsonr(xs, ys)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)\n",
        "\n",
        "## 5.4909917646575975e-17"
      ],
      "metadata": {
        "id": "AFoUbLr_ymNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intrinsic Dimension"
      ],
      "metadata": {
        "id": "9X6lC_5SzaD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The intrinsic dimension of a dataset is the number of features required to approximate it. The intrinsic dimension informs dimension reduction,\n",
        "because it tells us how much a dataset can be compressed.\n",
        "\n",
        "\n",
        "Consider this dataset with 2 features: latitude and longitude. These two features might track the flight of an airplane, for example.\n",
        "his dataset is 2-dimensional, yet it turns out that it can be closely approximated using only one feature: the displacement along the flight path.\n",
        "This dataset is intrinsically one-dimensional.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HJ81rELazcW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA features are ordered by variance**"
      ],
      "metadata": {
        "id": "FMqUacLY463O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "1) How Does PCA Find Intrinsic Dimension?\n",
        "---------------------------------------------\n",
        "\n",
        "PCA helps find the intrinsic dimension by sorting the PCA features based on variance.\n",
        "\n",
        "PCA rearranges the dataset so that the new features (PCA components) are ordered from most important to least important.\n",
        "The first few PCA components will have high variance, meaning they capture most of the data's structure.\n",
        "The remaining components have low variance, meaning they add little useful information.\n",
        "\n",
        "\n",
        "Key Idea: The number of PCA features with high variance tells us the intrinsic dimension of the dataset.\n",
        "\n",
        "\n",
        "2) How to Check Intrinsic Dimension?\n",
        "--------------------------------------------\n",
        "PCA provides a bar graph of variance for each PCA feature.\n",
        "\n",
        "The first few bars are tall → These are important PCA features.\n",
        "The last few bars are small → These features have low variance and can be ignored.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BsSGAzKi47LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### The first principal component\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "The first principal component of the data is the direction in which the data varies the most. In this exercise, job is to use PCA to find the first principal component\n",
        "of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Make a scatter plot of the grain measurements. This has been done for you.\n",
        "Create a PCA instance called model.\n",
        "Fit the model to the grains data.\n",
        "Extract the coordinates of the mean of the data using the .mean_ attribute of model.\n",
        "Get the first principal component of model using the .components_[0,:] attribute.\n",
        "Plot the first principal component as an arrow on the scatter plot, using the plt.arrow() function. You have to specify the first two arguments - mean[0] and mean[1].\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(grains[:,0], grains[:,1])\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(grains)\n",
        "\n",
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0,:]\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-fZHLtVxapPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Variance of the PCA features\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Create an instance of StandardScaler called scaler.\n",
        "Create a PCA instance called pca.\n",
        "Use the make_pipeline() function to create a pipeline chaining scaler and pca.\n",
        "Use the .fit() method of pipeline to fit it to the fish samples samples.\n",
        "Extract the number of components used using the .n_components_ attribute of pca. Place this inside a range() function and store the result as features.\n",
        "Use the plt.bar() function to plot the explained variances, with features on the x-axis and pca.explained_variance_ on the y-axis.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, pca)\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('variance')\n",
        "plt.xticks(features)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UWHwmSEKbHTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimension reduction with PCA"
      ],
      "metadata": {
        "id": "SBryJMUqb4kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "PCA discards the low variance features, and assumes that the higher variance features are informative\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kOB49cVZb7Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TruncatedSVD and csr_matrix**"
      ],
      "metadata": {
        "id": "ZxxjlKo8erQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "1) What is a Word Frequency Array?\n",
        "A word frequency array is a table where:\n",
        "\n",
        "Each row represents a document (e.g., a book, article, or review).\n",
        "Each column represents a word from a fixed list of words (vocabulary).\n",
        "The values in the table show how many times each word appears in each document.\n",
        "\n",
        "we have 3 documents and a vocabulary of 5 words:\n",
        "\n",
        "Document\t\"apple\"\t\"banana\"\t\"cat\"\t  \"dog\"\t\"elephant\"\n",
        "Doc 1       \t2\t      1\t      0\t      0\t    0\n",
        "Doc 2\t        0\t      3\t      0\t      1\t    0\n",
        "Doc 3\t        0\t      0\t      2\t      4\t    1\n",
        "\n",
        "\n",
        "Doc 1 mentions \"apple\" twice and \"banana\" once.\n",
        "Doc 2 mentions \"banana\" three times and \"dog\" once.\n",
        "Doc 3 mentions \"cat\", \"dog\", and \"elephant\".\n",
        "\n",
        "Key point:\n",
        "Most words do not appear in each document, so most of the values are 0. This makes the matrix sparse.\n",
        "\n",
        "2) What is a Sparse Matrix? (csr_matrix)\n",
        "--------------------------------------------\n",
        "A sparse matrix is a table where most values are zero.\n",
        "\n",
        "Instead of storing all the values, a sparse matrix only stores the non-zero values to save space.\n",
        "In Python, we use a special format called csr_matrix (Compressed Sparse Row matrix) to handle sparse data efficiently.\n",
        "\n",
        "\n",
        "\n",
        "Instead of storing:\n",
        "\n",
        "[2, 1, 0, 0, 0]\n",
        "[0, 3, 0, 1, 0]\n",
        "[0, 0, 2, 4, 1]\n",
        "\n",
        "\n",
        "A sparse matrix stores:\n",
        "\n",
        "(values) → [2, 1, 3, 1, 2, 4, 1]\n",
        "(positions) → [(0,0), (0,1), (1,1), (1,3), (2,2), (2,3), (2,4)]\n",
        "This method saves memory when dealing with large text datasets.\n",
        "\n",
        "\n",
        "\n",
        "3) Why Can’t We Use PCA on a Sparse Matrix?\n",
        "--------------------------------------------------\n",
        "PCA (Principal Component Analysis) does not work directly on csr_matrix because it expects a dense matrix (a regular table with all values).\n",
        "\n",
        "If we try to use PCA on a word frequency array, it will be slow and use too much memory.\n",
        "\n",
        "\n",
        "\n",
        "4) How Does TruncatedSVD Solve This Problem?\n",
        "-----------------------------------------------\n",
        "TruncatedSVD (Singular Value Decomposition) is like PCA but works with sparse matrices.\n",
        "\n",
        "It finds patterns in word frequency data.\n",
        "It helps reduce the number of features while keeping important information.\n",
        "It is faster and memory-efficient than PCA.\n",
        "\n",
        "    TruncatedSVD = PCA for sparse data\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LqWkP7Tbet4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import PCA from sklearn.decomposition.\n",
        "Create a PCA instance called pca with n_components=2.\n",
        "Use the .fit() method of pca to fit it to the scaled fish measurements scaled_samples.\n",
        "Use the .transform() method of pca to transform the scaled_samples. Assign the result to pca_features.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA model with 2 components: pca\n",
        "pca = PCA(n_components = 2)\n",
        "\n",
        "# Fit the PCA instance to the scaled samples\n",
        "pca.fit(scaled_samples)\n",
        "\n",
        "# Transform the scaled samples: pca_features\n",
        "pca_features = pca.transform(scaled_samples)\n",
        "\n",
        "# Print the shape of pca_features\n",
        "print(pca_features.shape)\n"
      ],
      "metadata": {
        "id": "V4blLBRzf4YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### A tf-idf word-frequency array\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import TfidfVectorizer from sklearn.feature_extraction.text.\n",
        "Create a TfidfVectorizer instance called tfidf.\n",
        "Apply .fit_transform() method of tfidf to documents and assign the result to csr_mat. This is a word-frequency array in csr_matrix format.\n",
        "Inspect csr_mat by calling its .toarray() method and printing the result. This has been done for you.\n",
        "The columns of the array correspond to words. Get the list of words by calling the .get_feature_names() method of tfidf, and assign the result to words\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer: tfidf\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Apply fit_transform to document: csr_mat\n",
        "csr_mat = tfidf.fit_transform(documents)\n",
        "\n",
        "# Print result of toarray() method\n",
        "print(csr_mat.toarray())\n",
        "\n",
        "# Get the words: words\n",
        "words = tfidf.get_feature_names()\n",
        "\n",
        "# Print words\n",
        "print(words)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "[[0.51785612 0.         0.         0.68091856 0.51785612 0.        ]\n",
        " [0.         0.         0.51785612 0.         0.51785612 0.68091856]\n",
        " [0.51785612 0.68091856 0.51785612 0.         0.         0.        ]]\n",
        "['cats', 'chase', 'dogs', 'meow', 'say', 'woof']\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5NURdwVZg-ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Clustering Wikipedia part I\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. Combine knowledge of TruncatedSVD and k-means\n",
        "to cluster some popular pages from Wikipedia. In this exercise, build the pipeline.\n",
        "In the next exercise, apply it to the word-frequency array of some Wikipedia articles.\n",
        "\n",
        "Create a Pipeline object consisting of a TruncatedSVD followed by KMeans. (This time, we've precomputed the word-frequency matrix for you, so there's no need for a TfidfVectorizer).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import:\n",
        "TruncatedSVD from sklearn.decomposition.\n",
        "KMeans from sklearn.cluster.\n",
        "make_pipeline from sklearn.pipeline.\n",
        "\n",
        "Create a TruncatedSVD instance called svd with n_components=50.\n",
        "Create a KMeans instance called kmeans with n_clusters=6.\n",
        "Create a pipeline called pipeline consisting of svd and kmeans\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a TruncatedSVD instance: svd\n",
        "svd = TruncatedSVD(n_components = 50)\n",
        "\n",
        "# Create a KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters = 6)\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(svd, kmeans)\n"
      ],
      "metadata": {
        "id": "SeJ5_iCMiMul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Clustering Wikipedia part II\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "It is now time to put your pipeline from the previous exercise to work! You are given an array articles of tf-idf word-frequencies of some popular Wikipedia articles,\n",
        "and a list titles of their titles. Use your pipeline to cluster the Wikipedia articles.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import pandas as pd\n",
        "Fit the pipeline to the word-frequency array articles.\n",
        "Predict the cluster labels.\n",
        "Align the cluster labels with the list titles of article titles by creating a DataFrame df with labels and titles as columns. This has been done for you.\n",
        "Use the .sort_values() method of df to sort the DataFrame by the 'label' column, and print the result.\n",
        "Hit submit and take a moment to investigate your amazing clustering of Wikipedia pages!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to articles\n",
        "pipeline.fit(articles)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(articles)\n",
        "\n",
        "# Create a DataFrame aligning labels and titles: df\n",
        "df = pd.DataFrame({'label': labels, 'article': titles})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('label'))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "label                                        article\n",
        "59      0                                    Adam Levine\n",
        "57      0                          Red Hot Chili Peppers\n",
        "56      0                                       Skrillex\n",
        "55      0                                  Black Sabbath\n",
        "54      0                                 Arctic Monkeys\n",
        "53      0                                   Stevie Nicks\n",
        "52      0                                     The Wanted\n",
        "51      0                                     Nate Ruess\n",
        "50      0                                   Chad Kroeger\n",
        "58      0                                         Sepsis\n",
        "30      1                  France national football team\n",
        "31      1                              Cristiano Ronaldo\n",
        "32      1                                   Arsenal F.C.\n",
        "33      1                                 Radamel Falcao\n",
        "37      1                                       Football\n",
        "35      1                Colombia national football team\n",
        "36      1              2014 FIFA World Cup qualification\n",
        "38      1                                         Neymar\n",
        "39      1                                  Franck Ribéry\n",
        "34      1                             Zlatan Ibrahimović\n",
        "26      2                                     Mila Kunis\n",
        "28      2                                  Anne Hathaway\n",
        "27      2                                 Dakota Fanning\n",
        "25      2                                  Russell Crowe\n",
        "29      2                               Jennifer Aniston\n",
        "23      2                           Catherine Zeta-Jones\n",
        "22      2                              Denzel Washington\n",
        "21      2                             Michael Fassbender\n",
        "20      2                                 Angelina Jolie\n",
        "24      2                                   Jessica Biel\n",
        "10      3                                 Global warming\n",
        "11      3       Nationally Appropriate Mitigation Action\n",
        "13      3                               Connie Hedegaard\n",
        "14      3                                 Climate change\n",
        "12      3                                   Nigel Lawson\n",
        "16      3                                        350.org\n",
        "17      3  Greenhouse gas emissions by the United States\n",
        "18      3  2010 United Nations Climate Change Conference\n",
        "19      3  2007 United Nations Climate Change Conference\n",
        "15      3                                 Kyoto Protocol\n",
        "8       4                                        Firefox\n",
        "1       4                                 Alexa Internet\n",
        "2       4                              Internet Explorer\n",
        "3       4                                    HTTP cookie\n",
        "4       4                                  Google Search\n",
        "5       4                                         Tumblr\n",
        "6       4                    Hypertext Transfer Protocol\n",
        "7       4                                  Social search\n",
        "49      4                                       Lymphoma\n",
        "42      4                                    Doxycycline\n",
        "47      4                                          Fever\n",
        "46      4                                     Prednisone\n",
        "44      4                                           Gout\n",
        "43      4                                       Leukemia\n",
        "9       4                                       LinkedIn\n",
        "48      4                                     Gabapentin\n",
        "0       4                                       HTTP 404\n",
        "45      5                                    Hepatitis C\n",
        "41      5                                    Hepatitis B\n",
        "40      5                                    Tonsillitis\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RpqxFttdjUXy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}