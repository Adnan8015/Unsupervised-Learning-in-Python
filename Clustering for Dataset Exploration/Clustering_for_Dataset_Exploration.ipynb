{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning"
      ],
      "metadata": {
        "id": "daEEDE5lqFcM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkSCcOOUpMZh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Unsupervised learning is a class of machine learning techniques for discovering patterns in data.\n",
        "For instance, finding the natural \"clusters\" of customers based on their purchase histories, or searching for patterns and correlations among these purchases,\n",
        "and using these patterns to express the data in a compressed form.\n",
        "\n",
        "These are examples of unsupervised learning techniques called \"clustering\" and \"dimension reduction\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cluster labels for new samples**"
      ],
      "metadata": {
        "id": "YGpXqexzYQ5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "After running K-means, the algorithm creates \"centroids\"—these are the average positions of each cluster in the data space.\n",
        "For example, if someone is grouping iris flowers by their petal length and width, each cluster will have a centroid which represents the average petal length\n",
        "and width of flowers in that group.\n",
        "\n",
        "\n",
        "\n",
        "Now, what happens if get new data (like new iris flowers)?\n",
        "----------------------------------------------------------====\n",
        "\n",
        "No need to run K-means from scratch on the new data. Instead, can use the existing centroids to assign these new samples to the closest cluster.\n",
        "\n",
        "Example:\n",
        "Suppose after clustering, one have 3 clusters with centroids at:\n",
        "Centroid 1: (5.1, 3.5)\n",
        "Centroid 2: (6.2, 2.8)\n",
        "Centroid 3: (5.8, 4.1)\n",
        "\n",
        "Now, if someone get a new sample, say a flower with petal length 5.6 and width 3.0, the algorithm will compare this sample to the centroids of each cluster\n",
        "and assign it to the cluster whose centroid is the closest.\n",
        "\n",
        "In this case, the new sample would be assigned to Centroid 1 because it's closest in terms of distance.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DleIQ-rtYSqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### How many clusters?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "You are given an array points of size 300x2, where each row gives the (x, y) co-ordinates of a point on a map. Make a scatter plot of these points,\n",
        "and use the scatter plot to guess how many clusters there are.\n",
        "\n",
        "matplotlib.pyplot has already been imported as plt.\n",
        "\n",
        "Create an array called xs that contains the values of points[:,0] - that is, column 0 of points.\n",
        "Create an array called ys that contains the values of points[:,1] - that is, column 1 of points.\n",
        "Make a scatter plot by passing xs and ys to the plt.scatter() function.\n",
        "Call the plt.show() function to show your plot.\n",
        "How many clusters do you see?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "xs = points[:,0]\n",
        "ys = points[:,1]\n",
        "plt.scatter(xs, ys)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WasN_H8TZSH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Clustering 2D points\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters.\n",
        "You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise.\n",
        "After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.\n",
        "\n",
        "\n",
        "You are given the array points from the previous exercise, and also an array new_points\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import KMeans from sklearn.cluster.\n",
        "Using KMeans(), create a KMeans instance called model to find 3 clusters. To specify the number of clusters, use the n_clusters keyword argument.\n",
        "Use the .fit() method of model to fit the model to the array of points points.\n",
        "Use the .predict() method of model to predict the cluster labels of new_points, assigning the result to labels.\n",
        "Hit submit to see the cluster labels of new_points\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a KMeans instance with 3 clusters: model\n",
        "model = KMeans(n_clusters = 3)\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(points)\n",
        "\n",
        "# Determine the cluster labels of new_points: labels\n",
        "labels = model.predict(new_points)\n",
        "\n",
        "# Print cluster labels of new_points\n",
        "print(labels)\n",
        "\n",
        "\n",
        "\n",
        "# [0 1 2 0 1 0 1 1 1 2 0 1 1 2 2 1 2 2 1 1 2 1 0 1 0 2 1 2 2 0 0 1 1 1 2 0 1\n",
        "#  1 0 1 2 0 0 2 0 1 2 2 1 1 1 1 2 2 0 0 2 2 2 0 0 1 1 1 0 1 2 1 0 2 0 0 0 1\n",
        "#  0 2 2 0 1 2 0 2 0 1 2 1 2 0 1 1 1 0 1 1 0 2 2 2 2 0 1 0 2 2 0 0 1 0 2 2 0\n",
        "#  2 2 2 1 1 1 1 2 2 1 0 1 2 1 0 2 1 2 2 1 2 1 2 0 1 0 0 1 2 0 1 0 0 2 1 1 0\n",
        "#  2 0 2 1 0 2 2 0 2 1 1 2 1 2 2 1 1 0 1 1 2 0 2 0 0 1 0 1 1 0 0 2 0 0 0 2 1\n",
        "#  1 0 2 0 2 2 1 1 1 0 1 1 1 2 2 0 1 0 0 0 2 1 1 1 1 1 1 2 2 1 2 2 2 2 1 2 2\n",
        "#  1 1 0 2 0 0 2 0 2 0 2 1 1 2 1 1 1 2 0 0 2 1 1 2 1 2 2 1 2 2 0 2 0 0 0 1 2\n",
        "#  2 2 0 1 0 2 0 2 2 1 0 0 0 2 1 1 1 0 1 2 2 1 0 0 2 0 0 2 0 1 0 2 2 2 2 1 2\n",
        "#  2 1 1 0]"
      ],
      "metadata": {
        "id": "gddEhtGaZpZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Inspect your clustering\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import matplotlib.pyplot as plt.\n",
        "Assign column 0 of new_points to xs, and column 1 of new_points to ys.\n",
        "Make a scatter plot of xs and ys, specifying the c=labels keyword arguments to color the points by their cluster label. Also specify alpha=0.5.\n",
        "Compute the coordinates of the centroids using the .cluster_centers_ attribute of model.\n",
        "Assign column 0 of centroids to centroids_x, and column 1 of centroids to centroids_y.\n",
        "Make a scatter plot of centroids_x and centroids_y, using 'D' (a diamond) as a marker by specifying the marker parameter. Set the size of the markers to be 50 using s=50\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assign the columns of new_points: xs and ys\n",
        "xs = new_points[:,0]\n",
        "ys = new_points[:,1]\n",
        "\n",
        "# Make a scatter plot of xs and ys, using labels to define the colors\n",
        "plt.scatter(xs, ys, c=labels, alpha = 0.5)\n",
        "\n",
        "# Assign the cluster centers: centroids\n",
        "centroids = model.cluster_centers_\n",
        "\n",
        "# Assign the columns of centroids: centroids_x, centroids_y\n",
        "centroids_x = centroids[:,0]\n",
        "centroids_y = centroids[:,1]\n",
        "\n",
        "# Make a scatter plot of centroids_x and centroids_y\n",
        "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "LYwPyQ2tar7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating a Clustering"
      ],
      "metadata": {
        "id": "SdZZtlklb2NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "1) Comparing Clusters with Known Labels\n",
        "\n",
        "If we already have correct labels (like the actual species of iris flowers: Setosa, Versicolor, and Virginica), we can compare our clusters with these labels.\n",
        "\n",
        "Suppose our K-means clustering groups flowers into three clusters. We can check if each cluster mostly contains one species.\n",
        "    If Cluster 1 has mostly Setosa, Cluster 2 has mostly Versicolor, and Cluster 3 has mostly Virginica, our clustering is good!\n",
        "    If clusters mix up species randomly, it's a sign that the clustering is not working well.\n",
        "\n",
        "\n",
        "\n",
        "2) Measuring Quality Without Labels\n",
        "\n",
        "But what if we don't have actual species labels?\n",
        "We need a way to measure clustering quality without using correct answers.\n",
        "There are techniques like inertia (within-cluster distance) and silhouette score, which help us decide if clusters are well-separated and compact.\n",
        "\n",
        "\n",
        "3) Choosing the Right Number of Clusters\n",
        "A good quality measure helps us decide how many clusters we should use.\n",
        "We can try different numbers of clusters (2, 3, 4…) and see which gives the best separation.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8Aa5wy0Bb5P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inertia measures clustering quality**"
      ],
      "metadata": {
        "id": "RouKtR_UH3Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "1) What Makes a Good Clustering?\n",
        "---------------------------------------\n",
        "A good clustering means that data points in the same cluster are close together (not spread out too much).\n",
        "\n",
        "\n",
        "2) What is Inertia?\n",
        "-----------------------\n",
        "Inertia measures how far data points are from the center (centroid) of their cluster.\n",
        "\n",
        "Lower inertia is better because it means points are closer to their centroid, making clusters tighter.\n",
        "\n",
        "\n",
        "NOTE: kmeans aims to place the clusters in a way that minimizes the inertia.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fNPniFudH6ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How many clusters to choose?**"
      ],
      "metadata": {
        "id": "o9PZyuxrITmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Ultimately, this is a trade-off. A good clustering has tight clusters (meaning low inertia). But it also doesn't have too many clusters.\n",
        "A good rule of thumb is to choose an elbow in the inertia plot, that is, a point where the inertia begins to decrease more slowly.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "64eqVXaLIVkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### How many clusters of grain?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "For each of the given values of k, perform the following steps:\n",
        "Create a KMeans instance called model with k clusters.\n",
        "Fit the model to the grain data samples.\n",
        "Append the value of the inertia_ attribute of model to the list inertias.\n",
        "The code to plot ks vs inertias has been written for you, so hit submit to see the plot!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "ks = range(1, 6)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters = k)\n",
        "\n",
        "    # Fit model to samples\n",
        "    model.fit(samples)\n",
        "\n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)\n",
        "\n",
        "# Plot ks vs inertias\n",
        "plt.plot(ks, inertias, '-o')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q4MG6KIAIha2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluating the grain clustering\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Create a KMeans model called model with 3 clusters.\n",
        "Use the .fit_predict() method of model to fit it to samples and derive the cluster labels. Using .fit_predict() is the same as using .fit() followed by .predict().\n",
        "Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.\n",
        "Use the pd.crosstab() function on df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label. Assign the result to ct.\n",
        "Hit submit to see the cross-tabulation!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create a KMeans model with 3 clusters: model\n",
        "model = KMeans(n_clusters = 3)\n",
        "\n",
        "# Use fit_predict to fit model and obtain cluster labels: labels\n",
        "labels = model.fit_predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "metadata": {
        "id": "YHM-IsU4Jbt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming features for better clusterings"
      ],
      "metadata": {
        "id": "PBsNHG44JzOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Variances**"
      ],
      "metadata": {
        "id": "wszHWAmtMiW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "1) Problem: K-Means Clusters Don’t Match Wine Varieties\n",
        "-----------------------------------------------------------\n",
        "We want to group three types of wine using K-Means, but when we check the results, the clusters don’t match the actual wine varieties.\n",
        "\n",
        " Ideal Case: Each wine variety gets its own cluster.\n",
        " What Happened: The clustering is incorrect, and different wine varieties are mixed in the same cluster.\n",
        "\n",
        "\n",
        "\n",
        "2) Why? Some Features Have Higher Variance Than Others\n",
        "------------------------------------------------------------\n",
        "Variance means how spread out a feature's values are.\n",
        "If one feature has large values and another has small values, K-Means will give more importance to the large-value feature, which can lead to bad clusters.\n",
        "Example:\n",
        "In the wine dataset, Malic Acid has a high variance (its values are spread out).\n",
        "Another feature, OD280, has a low variance (its values are closer together).\n",
        "Since K-Means uses distances, it will pay more attention to Malic Acid and ignore OD280, causing incorrect clusters.\n",
        "\n",
        "\n",
        "\n",
        "3) Solution: Standardize the Features Using StandardScaler\n",
        "To fix this, we can use StandardScaler which makes all features have:\n",
        "      Mean = 0\n",
        "      Variance = 1\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Jfxq8S1fJ2DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import:\n",
        "make_pipeline from sklearn.pipeline.\n",
        "StandardScaler from sklearn.preprocessing.\n",
        "KMeans from sklearn.cluster.\n",
        "\n",
        "\n",
        "Create an instance of StandardScaler called scaler.\n",
        "Create an instance of KMeans with 4 clusters called kmeans.\n",
        "Create a pipeline called pipeline that chains scaler and kmeans. To do this, you just need to pass them in as arguments to make_pipeline().\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters = 4)\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler , kmeans)\n"
      ],
      "metadata": {
        "id": "WttcgGhiP0pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import pandas as pd.\n",
        "Fit the pipeline to the fish measurements samples.\n",
        "Obtain the cluster labels for samples by using the .predict() method of pipeline.\n",
        "Using pd.DataFrame(), create a DataFrame df with two columns named 'labels' and 'species', using labels and species, respectively, for the column values.\n",
        "Using pd.crosstab(), create a cross-tabulation ct of df['labels'] and df['species']\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to samples\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and species as columns: df\n",
        "df = pd.DataFrame({'labels':labels , 'species':species})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'] , df['species'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "metadata": {
        "id": "hJgQCT8jQsad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Clustering stocks using KMeans\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "In this exercise, we'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day).\n",
        "We are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company,\n",
        "and each column corresponds to a trading day.\n",
        "\n",
        "Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of pipeline.\n",
        "The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n",
        "\n",
        "Normalizer() is different to StandardScaler(), Normalizer() rescales each sample - here, each company's stock price - independently of the other.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import Normalizer from sklearn.preprocessing.\n",
        "Create an instance of Normalizer called normalizer.\n",
        "Create an instance of KMeans called kmeans with 10 clusters.\n",
        "Using make_pipeline(), create a pipeline called pipeline that chains normalizer and kmeans.\n",
        "Fit the pipeline to the movements array.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import Normalizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create a normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a KMeans model with 10 clusters: kmeans\n",
        "kmeans = KMeans(n_clusters = 10)\n",
        "\n",
        "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
        "pipeline = make_pipeline(normalizer, kmeans)\n",
        "\n",
        "# Fit pipeline to the daily price movements\n",
        "pipeline.fit(movements)\n",
        "\n",
        "\n",
        "\n",
        "# Pipeline(steps=[('normalizer', Normalizer()),\n",
        "#                 ('kmeans', KMeans(n_clusters=10))])"
      ],
      "metadata": {
        "id": "87HKcxAOSKOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Which stocks move together?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "In the previous exercise, we clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way?\n",
        "We'll now inspect the cluster labels from your clustering to find out\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import pandas as pd.\n",
        "Use the .predict() method of the pipeline to predict the labels for movements.\n",
        "Align the cluster labels with the list of company names companies by creating a DataFrame df with labels and companies as columns. This has been done for you.\n",
        "Use the .sort_values() method of df to sort the DataFrame by the 'labels' column, and print the result.\n",
        "Hit submit and take a moment to see which companies are together in each cluster!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Predict the cluster labels: labels\n",
        "labels = pipeline.predict(movements)\n",
        "\n",
        "# Create a DataFrame aligning labels and companies: df\n",
        "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('labels'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# labels                           companies\n",
        "# 59       0                               Yahoo\n",
        "# 15       0                                Ford\n",
        "# 35       0                            Navistar\n",
        "# 26       1                      JPMorgan Chase\n",
        "# 16       1                   General Electrics\n",
        "# 58       1                               Xerox\n",
        "# 11       1                               Cisco\n",
        "# 18       1                       Goldman Sachs\n",
        "# 20       1                          Home Depot\n",
        "# 5        1                     Bank of America\n",
        "# 3        1                    American express\n",
        "# 55       1                         Wells Fargo\n",
        "# 1        1                                 AIG\n",
        "# 38       2                               Pepsi\n",
        "# 40       2                      Procter Gamble\n",
        "# 28       2                           Coca Cola\n",
        "# 27       2                      Kimberly-Clark\n",
        "# 9        2                   Colgate-Palmolive\n",
        "# 54       3                            Walgreen\n",
        "# 36       3                    Northrop Grumman\n",
        "# 29       3                     Lookheed Martin\n",
        "# 4        3                              Boeing\n",
        "# 0        4                               Apple\n",
        "# 47       4                            Symantec\n",
        "# 33       4                           Microsoft\n",
        "# 32       4                                  3M\n",
        "# 31       4                           McDonalds\n",
        "# 30       4                          MasterCard\n",
        "# 50       4  Taiwan Semiconductor Manufacturing\n",
        "# 14       4                                Dell\n",
        "# 17       4                     Google/Alphabet\n",
        "# 24       4                               Intel\n",
        "# 23       4                                 IBM\n",
        "# 2        4                              Amazon\n",
        "# 51       4                   Texas instruments\n",
        "# 43       4                                 SAP\n",
        "# 45       5                                Sony\n",
        "# 48       5                              Toyota\n",
        "# 21       5                               Honda\n",
        "# 22       5                                  HP\n",
        "# 34       5                          Mitsubishi\n",
        "# 7        5                               Canon\n",
        "# 56       6                            Wal-Mart\n",
        "# 57       7                               Exxon\n",
        "# 44       7                        Schlumberger\n",
        "# 8        7                         Caterpillar\n",
        "# 10       7                      ConocoPhillips\n",
        "# 12       7                             Chevron\n",
        "# 13       7                   DuPont de Nemours\n",
        "# 53       7                       Valero Energy\n",
        "# 39       8                              Pfizer\n",
        "# 41       8                       Philip Morris\n",
        "# 25       8                   Johnson & Johnson\n",
        "# 49       9                               Total\n",
        "# 46       9                      Sanofi-Aventis\n",
        "# 37       9                            Novartis\n",
        "# 42       9                   Royal Dutch Shell\n",
        "# 19       9                     GlaxoSmithKline\n",
        "# 52       9                            Unilever\n",
        "# 6        9            British American Tobacco"
      ],
      "metadata": {
        "id": "P2bVV8ItTU3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}